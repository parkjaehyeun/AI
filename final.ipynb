{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK3oAfCzTRtd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeXOhipQyjna"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Cifar10.zip -d /content/drive/MyDrive/Cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1sI0nbfef1T"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, optimizers, datasets, preprocessing, utils\n",
        "\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yce61e5Fctxu"
      },
      "source": [
        "groups_folder_path = '/content/drive/MyDrive/Cifar10/'\n",
        "categories = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "num_classes = len(categories)\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "image_w = 32\n",
        "image_h = 32\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for idex, categorie in enumerate(categories):\n",
        "    label = [0 for i in range(num_classes)]\n",
        "    label[idex] = 1\n",
        "    image_dir = groups_folder_path + categorie + '/'\n",
        "\n",
        "    for top, dir, f in os.walk(image_dir):\n",
        "        for filename in f:\n",
        "            #print(image_dir+filename) \n",
        "            img = cv2.imread(image_dir+filename) #imread : 파일 하나 읽어주기\n",
        "            img = cv2.resize(img, None, fx=image_w/img.shape[0], fy=image_h/img.shape[1]) # 여기서 normalizing해줌\n",
        "            X.append(img/255)\n",
        "            Y.append(label)\n",
        " \n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLb9gk-ByXW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Activation, Dense\n",
        "from keras.layers import Flatten, Convolution2D, MaxPooling2D\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train_data, X_test_data, Y_train_label, Y_test_label = train_test_split(X,Y, stratify=Y) #stratify를 통해서 한 쪽에 쏠려서 분배되는 것을 방지\n",
        "xy = (X_train_data, X_test_data, Y_train_label, Y_test_label)\n",
        "\n",
        "np.save(\"./img_data.npy\", xy) # train과 test set으로 나누어진 파일을 numpy 파일에 넣어줌\n",
        "X_train_data.shape, X_test_data.shape, Y_train_label.shape, Y_test_label.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUkBz2DEL_W"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, optimizers, datasets, preprocessing, utils\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "input_layer = layers.Input((32, 32, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n55AIn_IeJY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model = keras.models.Sequential( [\n",
        "    keras.layers.Conv2D(input_shape = (32, 32, 3),\n",
        "                        kernel_size = (4,4), padding = 'same', \n",
        "                        filters = 32),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(kernel_size = (4,4), padding = 'same', \n",
        "                        filters = 32),\n",
        "    keras.layers.BatchNormalization(),\n",
        "     keras.layers.MaxPooling2D((2, 2)), # maxpooling크기 변경 / stride 변경\n",
        "     keras.layers.Dropout(rate=0.4),\n",
        "     keras.layers.Conv2D(kernel_size = (3,3), padding ='same', \n",
        "                        filters = 64),\n",
        "     keras.layers.BatchNormalization(),\n",
        "     keras.layers.Conv2D(kernel_size = (3,3), padding ='same', \n",
        "                        filters = 64),\n",
        "     keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(rate=0.4),\n",
        "    keras.layers.Conv2D(kernel_size = (3,3), padding = 'same', \n",
        "                        filters = 128),\n",
        "     keras.layers.BatchNormalization(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation = 'relu'),\n",
        "    keras.layers.Dense(32, activation = 'relu'),\n",
        "    keras.layers.Dense(10, activation = 'softmax'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8GdX4ZyIkQ8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm0RAINnIsv9"
      },
      "source": [
        "model.compile(\n",
        "    loss=losses.CategoricalCrossentropy(),\n",
        "    optimizer='adam',\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FRS7KuEIt7s"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train_data, Y_train_label,\n",
        "    validation_data=(X_test_data, Y_test_label),\n",
        "    batch_size=256,\n",
        "    epochs=50\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhPZ6NgOlnDy"
      },
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "# batch normalization 2개에 하나씩 사용한 결과"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghrY8zHke2ll"
      },
      "source": [
        "plt.plot(history.history['loss'], 'b-') # 파란선 : 훈련용 데이터에 대한 손실\n",
        "plt.plot(history.history['val_loss'], 'r--') # 빨간선 : 검증용 데이터에 대한 손실"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQCLjKjUfPM5"
      },
      "source": [
        "plt.plot(history.history['accuracy'], 'b-') # 파란선 : 훈련용 데이터에 대한 정확도\n",
        "plt.plot(history.history['val_accuracy'], 'r--') # 빨간선 : 검증용 데이터에 대한 정확도"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_noD2JWaEQN"
      },
      "source": [
        ""
      ]
    }
  ]
}